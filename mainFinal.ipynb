{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quaternion convolutional layers reproduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly I want to check if it possible to recreate the paper. This process will consist of the following steps\n",
    "\n",
    "- Dataloader\n",
    "- DataFormatter\n",
    "- QCNN import \n",
    "- QNN import\n",
    "- Recreate archtitecture \n",
    "- Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions for the following session\n",
    "\n",
    "- Where does the conversion to quaternions happen\n",
    "- When we input the output feautures map into the Qlinearlayer we lose all our information right\n",
    "- Why do de images needed te be padded with a extra dimension\n",
    "\n",
    "# To do following session\n",
    "\n",
    "- Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import core_qnn as qcnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from   torch.nn import Parameter\n",
    "from   torch.nn import functional as F\n",
    "import torch.optim\n",
    "from   torch.autograd import Variable\n",
    "from   torch import autograd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "# Tranform data to PyTorch tensore\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "grayscale = torchvision.transforms.Grayscale(num_output_channels=1)\n",
    "\n",
    "def convert_data_for_quaternion(batch):\n",
    "    \"\"\"\n",
    "    converts batches of RGB images in 4 channels for QNNs\n",
    "    \"\"\"\n",
    "    assert all(batch[i][0].size(0) == 3 for i in range(len(batch)))\n",
    "    inputs, labels = [], []\n",
    "    for i in range(len(batch)):\n",
    "        inputs.append(torch.cat([batch[i][0], torch.zeros(grayscale(batch[i][0]).size())], 0))\n",
    "        labels.append(batch[i][1])\n",
    "    \n",
    "    return torch.stack(inputs), torch.LongTensor(labels)\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "    #  transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "     ])\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    #  transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "    ]\n",
    "     )\n",
    "\n",
    "data = datasets.CIFAR10(root='data', train=True, download=True, transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(data, batch_size=32, shuffle=True,collate_fn=convert_data_for_quaternion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the size of image data torch.Size([4, 32, 32])\n",
      "This is the size of labels data torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24a8ae294e0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iter_func = iter(loader)\n",
    "\n",
    "image_data, label_data = next(iter_func)\n",
    "\n",
    "print(\"This is the size of image data\",image_data[0].shape)\n",
    "\n",
    "print(\"This is the size of labels data\",label_data.shape)\n",
    "\n",
    "plt.imshow(image_data[0][:3,:,:].permute((2,1,0)), cmap='Greys_r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.5373,  ..., 0.5451, 0.5451, 0.5490],\n",
      "        [0.0000, 0.0000, 0.5255,  ..., 0.4941, 0.4941, 0.5020],\n",
      "        [0.0000, 0.0000, 0.4784,  ..., 0.4510, 0.4471, 0.4510]])\n"
     ]
    }
   ],
   "source": [
    "print(image_data[31][2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   core_qnn.quaternion_layers import QuaternionLinear, QuaternionTransposeConv, QuaternionConv\n",
    "\n",
    "class NormalModule(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NormalModule, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(3072, 2048)\n",
    "        self.layer2 = nn.Linear(2048, 516)\n",
    "        self.layer3 = nn.Linear(516, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = th.flatten(x)\n",
    "        x = self.layer1.forward(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2.forward(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3.forward(x)\n",
    "        x = F.softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"SimpleModule\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleModule, self).__init__()\n",
    "        \n",
    "        self.layer1 = QuaternionConv(in_channels=4, out_channels=4, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.layer2 = nn.Linear(4096, 516)\n",
    "        self.layer3 = nn.Linear(516, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1.forward(x)\n",
    "        x = F.relu(x)\n",
    "        x = th.flatten(x)\n",
    "        x = self.layer2.forward(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3.forward(x)\n",
    "        x = F.softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"SimpleModule\"\n",
    "    \n",
    "class TheModule(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TheModule, self).__init__()\n",
    "        \n",
    "        self.layer1 = QuaternionConv(in_channels=4, out_channels=32, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.layer2 = QuaternionConv(in_channels=32, out_channels=32, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.layer3 = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "        self.layer4 = nn.Dropout(0.25)\n",
    "\n",
    "        self.layer5 = QuaternionConv(in_channels=32, out_channels=64, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.layer6 = QuaternionConv(in_channels=64, out_channels=64, kernel_size = 3, stride = 1, padding = 1)\n",
    "\n",
    "        # self.layer7 = QuaternionLinear(4096, 512)\n",
    "        self.layer7 = nn.Linear(4096, 512)\n",
    "        \n",
    "        # self.layer8 = QuaternionLinear(512, 10)\n",
    "        self.layer8 = nn.Linear(512, 10)\n",
    "\n",
    "\n",
    "        self.layer9 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1.forward(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2.forward(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "        \n",
    "        x = self.layer5.forward(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer6.forward(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        # x = self.layer4(x) \n",
    "        \n",
    "        flat = nn.Flatten()\n",
    "        x = flat(x)\n",
    "        \n",
    "        x = self.layer7.forward(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer8.forward(x)\n",
    "        # x = self.layer9(x)\n",
    "        x = th.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"SimpleModule\"\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your model\n",
    "if False: model = SimpleModule().cuda(); BATCH_SIZE = 1\n",
    "if True: model = TheModule().cuda(); BATCH_SIZE = 1\n",
    "if False: model = NormalModule().cuda(); BATCH_SIZE = 1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr = 0.0001,weight_decay=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Loss: 2.0842\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     15\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 16\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     17\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     19\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[39m# Print progress\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 80\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in loader:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # print statistics\n",
    "            # Print progress\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        losses.append(loss.cpu().item())\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'\n",
    "            .format(epoch+1, num_epochs, loss.cpu().item()))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the data\n",
    "\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_test = datasets.CIFAR10(root='data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(data_test, batch_size=10000, shuffle=False, \\\n",
    "    collate_fn=convert_data_for_quaternion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "x = datetime.datetime.now()\n",
    "string = str(x.day) + \"_\" + str(x.hour) + \"_\" + str(x.minute) + \"_\" + str(x.second) \n",
    "torch.save(model.state_dict(), f\"./modelsParameters/{string}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TheModule(\n",
       "  (layer1): QuaternionConv(in_channels=1, out_channels=8, bias=True, kernel_size=(3, 3), stride=1, padding=1, init_criterion=glorot, weight_init=quaternion, seed=636, rotation=False, q_format=True, operation=convolution2d)\n",
       "  (layer2): QuaternionConv(in_channels=8, out_channels=8, bias=True, kernel_size=(3, 3), stride=1, padding=1, init_criterion=glorot, weight_init=quaternion, seed=282, rotation=False, q_format=True, operation=convolution2d)\n",
       "  (layer3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer4): Dropout(p=0.25, inplace=False)\n",
       "  (layer5): QuaternionConv(in_channels=8, out_channels=16, bias=True, kernel_size=(3, 3), stride=1, padding=1, init_criterion=glorot, weight_init=quaternion, seed=1001, rotation=False, q_format=True, operation=convolution2d)\n",
       "  (layer6): QuaternionConv(in_channels=16, out_channels=16, bias=True, kernel_size=(3, 3), stride=1, padding=1, init_criterion=glorot, weight_init=quaternion, seed=802, rotation=False, q_format=True, operation=convolution2d)\n",
       "  (layer7): QuaternionLinear(in_features=1024, out_features=128, bias=True, init_criterion=he, weight_init=quaternion, seed=841)\n",
       "  (layer8): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (layer9): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TheModule().cuda()\n",
    "model.load_state_dict(torch.load(\"./hoi\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 9.6100%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        images, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
